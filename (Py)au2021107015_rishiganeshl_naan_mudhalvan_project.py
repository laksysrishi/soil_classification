# -*- coding: utf-8 -*-
"""au2021107015_RishiGaneshL_Naan Mudhalvan Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nz3M6WcJBM0RP8kgtfwmickJ97W3YRFB
"""

# importing the necessities
import pandas as pd
import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import folium
from folium.plugins import FloatImage
from shapely.geometry import Point
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Load the data
df = pd.read_csv('/content/drive/MyDrive/Naan Mudhalvan/updated_data.csv')

# Data overview
print("Dataset Shape:", df.shape)
print("First 5 rows of the dataset:")
print(df.head())

# Descriptive statistics
print("Descriptive statistics of the dataset:")
print(df.describe())

#Data Overview - Visualizing the data points in a map

# Create a base map centered around Germany
m = folium.Map(location=[51.1657, 10.4515], zoom_start=5, tiles="OpenStreetMap")

# Get unique locations
unique_locations_ = df[['latitude', 'longitude']].drop_duplicates()
print("There are", len(unique_locations_), "unique locations in the dataset.")

# Add unique locations to the map
for idx, row in unique_locations_.iterrows():
    folium.CircleMarker(location=(row['latitude'], row['longitude']), radius=1,color='red').add_to(m)

#display the map
m

#Data preprocessing - Remove the points outside Germany border

# Define a boundary for Germany
germany_boundary = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))
germany_boundary = germany_boundary[germany_boundary.name == 'Germany']

# Create a GeoDataFrame containing the points
geometry = []

# Iterate over each row in the DataFrame
for index, row in unique_locations_.iterrows():
    # Extract longitude and latitude values from the current row
    longitude = row['longitude']
    latitude = row['latitude']

    # Create a Point object using the longitude and latitude
    point = Point(longitude, latitude)

    # Append the Point object to the list
    geometry.append(point)

points_gdf = gpd.GeoDataFrame(unique_locations_, geometry=geometry, crs=germany_boundary.crs)

# Perform spatial join to filter points within Germany boundary
points_within_germany = gpd.sjoin(points_gdf, germany_boundary, how="inner", op="within")

# Get the indices of points within Germany
indices_within_germany = points_within_germany.index

# Filter the original DataFrame based on the indices
df1 = df.loc[indices_within_germany]

# Display the filtered DataFrame
print("Filtered DataFrame with points only within Germany:")
print(df1.head())

# Data overview of filtered dataset
print("Dataset Shape:", df1.shape)
print("First 5 rows of the dataset:")
print(df1.head())

#Check for missing values
print("Missing values in each column:")
print(df1.isnull().sum())

# Handle missing values by either removing or imputing them
df1 = df1.dropna()

#Removal of duplicate records
df1.drop_duplicates()

#Feature Engineering

# Remove the 'time' column from the DataFrame
df1.drop(columns=['time'], inplace=True)

#Averaging the satellite moisture values
df1['sm_avg'] = (df1['sm_aux']+df1['sm_tgt'])/2

df1

# Detect outliers using z-scores
from scipy import stats

z_scores = np.abs(stats.zscore(df1))
threshold = 3
outliers = np.where(z_scores > threshold)

# Remove outliers from the DataFrame
df_no_outliers = df1[(z_scores < threshold).all(axis=1)]

print("Original DataFrame:")
print(df1)
print("\nDataFrame without outliers:")
print(df_no_outliers)

#Preprocessing - Data Normalization

from sklearn.preprocessing import MinMaxScaler

# Define the columns you want to normalize
columns_to_normalize = ['clay_content', 'sand_content', 'silt_content', 'sm_aux', 'sm_tgt', 'sm_avg']

# Initialize the MinMaxScaler
scaler = MinMaxScaler()

# Normalize the selected columns
df_normalized = df_no_outliers.copy()
df_normalized[columns_to_normalize] = scaler.fit_transform(df_normalized[columns_to_normalize])

print(df_normalized)

# Compute the correlation matrix
correlation_matrix = df_normalized.corr()

# Plot the correlation matrix as a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

#Style setting
sns.set_style("whitegrid")
sns.set_context("talk")

# Create the distribution plot
plt.figure(figsize=(12, 8))
sns.histplot(df_normalized['sm_avg'], bins=30, kde=True, color='blue', edgecolor='black')

plt.title("Distribution of Soil Moisture in Germany")
plt.xlabel("Soil Moisture Value", fontsize=14)
plt.ylabel("Frequency")

plt.tight_layout()
plt.show()

# Randomly sample 300 data points from the dataframe
sample_df = df_normalized.sample(n=300, random_state=42)

# Initialize a figure with three subplots side by side
fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 6))

# List of soil components
components = ['clay_content', 'sand_content', 'silt_content']

# Plot scatter plots for each component
for i, component in enumerate(components):
    sns.regplot(x=component, y='sm_avg', data=sample_df, ax=axes[i], color='skyblue', scatter_kws={'s':10}, line_kws={'color':'red'})

    # Calculate correlation and annotate the plot with its value
    correlation = sample_df[component].corr(sample_df['sm_avg'])
    axes[i].set_title(f"Correlation between {component.split('_')[0].capitalize()} and Soil Moisture: {correlation:.2f}", fontsize=14)

# Adjust layout for better display
plt.tight_layout()
plt.show()

#Feature Selection and Engineering

#Define Labels based on USDA Soil Taxonomy
def classify_soil(row):
    if row['clay_content'] < 0.20 and row['sand_content'] < 0.28 and row['silt_content'] >= 0.52:
        return 'silt'
    elif row['clay_content'] < 0.20 and row['sand_content'] >= 0.52:
        return 'sand'
    elif row['clay_content'] >= 0.20 and row['sand_content'] < 0.45 and row['silt_content'] >= 0.40:
        return 'sandy clay'
    elif row['clay_content'] < 0.7 and row['sand_content'] < 0.15 and row['silt_content'] < 0.52:
        return 'sandy loam'
    elif row['clay_content'] >= 0.7 and row['sand_content'] < 0.15 and row['silt_content'] < 0.52:
        return 'loamy sand'
    elif row['clay_content'] >= 0.7 and row['clay_content'] < 0.20 and row['sand_content'] >= 0.15 and row['sand_content'] < 0.28 and row['silt_content'] >= 0.52:
        return 'silt loam'
    elif row['clay_content'] < 0.7 and row['sand_content'] >= 0.15 and row['sand_content'] < 0.28 and row['silt_content'] < 0.52:
        return 'sandy clay loam'
    elif row['clay_content'] >= 0.7 and row['clay_content'] < 0.20 and row['sand_content'] < 0.15 and row['silt_content'] >= 0.52:
        return 'silty clay loam'
    elif row['clay_content'] >= 0.7 and row['clay_content'] < 0.20 and row['sand_content'] >= 0.15 and row['sand_content'] < 0.28 and row['silt_content'] < 0.52:
        return 'sandy clay loam'
    elif row['clay_content'] >= 0.20 and row['sand_content'] < 0.15 and row['silt_content'] >= 0.52:
        return 'clay loam'
    elif row['clay_content'] >= 0.20 and row['sand_content'] >= 0.15 and row['sand_content'] < 0.28 and row['silt_content'] >= 0.52:
        return 'silty clay loam'
    elif row['clay_content'] >= 0.20 and row['sand_content'] >= 0.28 and row['silt_content'] >= 0.52:
        return 'silty clay'
    elif row['clay_content'] >= 0.20 and row['sand_content'] < 0.52 and row['silt_content'] < 0.52:
        return 'clay'

# Apply classification
df_normalized['soil_texture'] = df_normalized.apply(classify_soil, axis=1)

print(df_normalized)

from sklearn.preprocessing import LabelEncoder

data = df_normalized.copy()

data = data.dropna()

# Select features for training from the original dataframe
selected_features = ['silt_content', 'sand_content', 'clay_content', 'sm_avg']

X = data[selected_features]  # Features (all columns except the target)
y = data['soil_texture']  # Target column


# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

from sklearn.preprocessing import LabelEncoder

# Initialize the label encoder
label_encoder = LabelEncoder()

# Encode the target variable
y_train_encoded = label_encoder.fit_transform(y_train)

# Initialize and train the RandomForestClassifier
forest = RandomForestClassifier(n_estimators=50, random_state=0)
forest.fit(X_train, y_train_encoded)

# Predict on the test set
y_pred_encoded = forest.predict(X_test)

# Decode the predicted labels
y_pred = label_encoder.inverse_transform(y_pred_encoded)

#Evaluation of Model

# Print accuracy on training and testing subsets
print('Accuracy on the training subset: {:.3f}'.format(forest.score(X_train, y_train_encoded)))
print('Accuracy on the testing subset: {:.3f}'.format(forest.score(X_test, y_pred_encoded)))

# Confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print('Confusion Matrix:')
print(conf_matrix)

# Classification report
class_report = classification_report(y_test, y_pred)
print('Classification Report:')
print(class_report)

#Comparison with SVM and Decision Trees

from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier

# Train SVM model
svm_model = SVC(kernel='linear', random_state=0)
svm_model.fit(X_train, y_train)

# Train Decision Trees model
dt_model = DecisionTreeClassifier(random_state=0)
dt_model.fit(X_train, y_train)

# Predictions
svm_pred = svm_model.predict(X_test)
dt_pred = dt_model.predict(X_test)

# Evaluate SVM model
svm_accuracy = accuracy_score(y_test, svm_pred)
svm_conf_matrix = confusion_matrix(y_test, svm_pred)
svm_report = classification_report(y_test, svm_pred)

# Evaluate Decision Trees model
dt_accuracy = accuracy_score(y_test, dt_pred)
dt_conf_matrix = confusion_matrix(y_test, dt_pred)
dt_report = classification_report(y_test, dt_pred)

# Print performance metrics
print("SVM Performance Metrics:")
print("Accuracy:", svm_accuracy)
print("Confusion Matrix:\n", svm_conf_matrix)
print("Report:\n", svm_report)

print("\nDecision Trees Performance Metrics:")
print("Accuracy:", dt_accuracy)
print("Confusion Matrix:\n", dt_conf_matrix)
print("Report:\n", dt_report)

# Define colors for each class
class_colors = {
    'sand': 'red',
    'loamy sand': 'pink',
    'sandy loam': 'orange',
    'silt loam': 'lightgreen',
    'sandy clay loam': 'peach',
    'silty clay loam': 'yellow',
    'silty clay': 'blue',
    'clay loam': 'violet',
    'sandy clay': 'cyan',
    'clay': 'darkgreen',
    'silt': 'darkblue'
    # Add more colors for additional classes if needed
}

# Get unique classes from the data
unique_classes = data['soil_texture'].unique()
# Create a map centered at the mean of latitude and longitude
m = folium.Map(location=[data['latitude'].mean(), data['longitude'].mean()], zoom_start=10)

# Add markers for each data point with color corresponding to its class
for index, row in data.iterrows():
    class_color = class_colors.get(row['soil_texture'], 'black')  # Default to black if class not found
    folium.CircleMarker(location=[row['latitude'], row['longitude']], color=class_color, radius =1, fill_color=class_color).add_to(m)

# Create legend only for existing points on the map
legend_html = """
     <div style="position: fixed;
                 bottom: 50px; left: 50px; width: 180px; height: auto;
                 border:2px solid grey; z-index:9999; font-size:14px;
                 background-color: rgba(255, 255, 255, 0.8);
                ">
     <p style="margin: 10px; color:black;"><strong>Legend</strong></p>
     """
for cls in unique_classes:
    class_color = class_colors.get(cls, 'black')  # Default to black if class not found
    legend_html += f'<p style="margin: 5px; color:{class_color};">{cls}</p>'
legend_html += """
      </div>
     """

m.get_root().html.add_child(folium.Element(legend_html))

# Display the map
m